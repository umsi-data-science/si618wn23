{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# SI 618: Data Manipulation and Analysis\n",
    "## 03 - Pandas 2\n",
    "### Dr. Chris Teplovs, School of Information, University of Michigan\n",
    "\n",
    "<small><a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a> This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n",
    "    \n",
    "Version 2023.01.17.1.CT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before you start**: use pip to install the `lxml` package (i.e. use the terminal to do `pip install lxml`. Please see the class Slack channel for hints if you have problems installing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### IMPORTANT: Replace ```?``` in the following code with your uniqname."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_UNIQNAME = '?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and manipulating data in pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "* load CSV files\n",
    "* load JSON files\n",
    "* use pd.read_html to extract tables from web pages\n",
    "* handle missing data (dropna and fillna)\n",
    "* use vectorized string functions\n",
    "* use Pandas' apply function to run a function on each row of a dataframe\n",
    "* view and set the indexes of a dataframe, including hierarchical indexes\n",
    "* use loc to explore a dataframe with hierarchical indexes\n",
    "* use stack and unstack to reshape dataframes\n",
    "* concatenate two DataFrames by columns\n",
    "* use Pandas' merge function to join dataframes in a SQL-like way\n",
    "* use the .describe() function\n",
    "* understand .groupby()\n",
    "* know how to use pivot and pivot_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the ```pd.read_csv``` function that we used to load data sets in previous classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = pd.read_csv('https://github.com/umsi-data-science/data/raw/main/titles.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That works great for well-formatted CSV files, but what happens when you get something that looks like the ```data/avocado_eu.csv``` file.\n",
    "\n",
    "If you're using JupyterLab, go ahead and browse that in JupyterLab's CSV browser.  You'll notice a new drop-down menu labelled \"Delimiter\".  Go ahead and change that to ```;```.\n",
    "\n",
    "Referring back to your readings and the [read_csv documentation online](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html), complete the following exercise\n",
    "\n",
    "Read the data/avocado_eu.csv file into a pandas DataFrame and show the first 5 rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avocado = pd.read_csv('https://github.com/umsi-data-science/data/raw/main/avocado_eu.csv')\n",
    "avocado.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that, unless you did something special in the previous read_csv invocation, the decimal points don't look quite right.  Go ahead and find the right option to convert commas to periods when loading a CSV file.\n",
    "\n",
    "## <font color=\"magenta\">Q1 (2 points): \n",
    "Read the avocado_eu.csv file into a DataFrame called \"avocado\" using the correct delimiter and decimal character into a dataframe and show the first 5 rows: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting the number of values (review)\n",
    "\n",
    "Sometimes, you'll want to count the number of times values occur.  For example, we might want to know the number of times each 'type'\n",
    "is reported in our avocado data.  Use the ```value_counts()``` function on a Series to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avocado['type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading JSON data\n",
    "\n",
    "In addition to CSV files, JSON (JavaScript Object Notation) files or data is commonly used.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_football_players = pd.read_json('https://github.com/umsi-data-science/data/raw/main/nfl_football_profiles.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_football_players.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, just for fun, show the player with the highest Current Salary from that dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_football_players.sort_values('current_salary', ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing up the data\n",
    "Assuming you did something like sort_values on one of the original columns, you probably got the wrong result.\n",
    "\n",
    "Looking a bit more closely at the results, you'll notice that the current_salary column.  Remembering that we have made the shift from pythonic to pandorable, we can leverage the impressive-sounding \"vectorized string functions\" mentioned in Section 7.3 of the McKinney book.  Specifically, we can use the str.replace(...) method.  Note that had we use read_csv to load the file we could have used the ```thousands=``` option and avoided all this, but sometimes data doesn't come in a convenient format.\n",
    "\n",
    "One way to apply functions is to operate on a column and then assign the results to another column.  For example, if we wanted to eliminate commas, we could replace them with null strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_football_players['current_salary'].str.replace(',', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And assign the results to a column in the original dataframe (in this case I'm calling the column current_salary_nocommas). We need to assign it to a column to ensure the operation is done in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_football_players['current_salary_nocommas'] = \\\n",
    "    nfl_football_players['current_salary'].str.replace(',', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_football_players['current_salary_nocommas'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But you'll notice that the type of the column is string, and we want to convert it to a float so we can sort it numerically.  So we can use the astype() function to convert it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_football_players['current_salary_cleaned'] = \\\n",
    "    nfl_football_players['current_salary'].str.replace(',', '').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_football_players.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can re-run our command to sort by salary and get the correct result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_football_players.sort_values('current_salary_cleaned', ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping missing values\n",
    "\n",
    "In addition to the \"all\" or \"any\" functionality described in McKinney section 7.1, it's sometimes useful to drop a row only if a certain column or columns have missing data.  To do this, use the subset= option with dropna().  So, for example, to drop all players for whom we do not have salary information, we could use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_football_players.current_salary_cleaned.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_football_players_salaries = nfl_football_players.dropna(subset=['current_salary_cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = nfl_football_players['current_salary_cleaned'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_football_players_salaries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dummy variables\n",
    "\n",
    "We might, on occasion, want to \"bin\" or \"discretize\" a variable.  For example, we might want to take the previous dataframe and add dummy variables that map onto whether the salaries are \"small\" (< \\\\$1M) , \"medium\" (\\\\$1M - \\\\$10M), or \"large\" (> \\\\$10M). A **dummy variable** is a binary indicator variable (often used in regressions) to represent a categorical variable. \n",
    "\n",
    "We could do something like the following (I encourage you to make detailed notes about the following lines; we do several important steps.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0,1000000,10000000,1000000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.cut(nfl_football_players_salaries['current_salary_cleaned'],bins,labels=['small','medium','large'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(pd.cut(nfl_football_players_salaries['current_salary_cleaned'],bins,labels=['small','medium','large']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.concat stacks together objects along an axis _(Section 8.2 McKinney)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_cats = pd.concat([nfl_football_players_salaries,dummies],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nfl_cats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"magenta\">Q2 (12 points, 10 minutes): \n",
    "Create a new dataframe that contains all the columns in the nfl_football_players dataframe as well as an additional column that contains each player's height in centimeters. Show the first 5 rows of your result. </font>\n",
    "\n",
    "**Hints:** \n",
    "- 1 inch = 2.54 cm\n",
    "- you can use the vectorized string function str.split() to separate feet and inches from the original dataframe column\n",
    "- remember to cast strings to numeric types if you're going to perform math on them\n",
    "- you might want to create an intermediate (temporary) DataFrame to help you keep things clear instead of attempting to do \n",
    "this in one line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Code Here. You should use multiple code boxes. You can add them below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Tables from HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```pd.read_html``` function returns a list of DataFrames read from an HTML source.  The following line will return a list of DataFrames from https://en.wikipedia.org/wiki/List_of_largest_sports_contracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_scraped = pd.read_html('https://en.wikipedia.org/wiki/List_of_largest_sports_contracts', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(contracts_scraped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the first table, you'll need to pull off the 0th element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts = contracts_scraped[0]\n",
    "contracts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"magenta\"> Q3 (2 points): \n",
    "Count the number of players from each sport in the List of Largest Sports Contracts. \n",
    "Hint:  see value_counts() description above\n",
    "<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Insert English explanation of findings here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining, Combining, and Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frodo_url = 'https://en.wikipedia.org/wiki/Frodo_Baggins'\n",
    "frodo_tables = pd.read_html(frodo_url)\n",
    "frodo_tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frodo_tables[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a function that, given a Wikipedia URL, will extract the contents of the Aliases component of the infobox table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aliases(url):\n",
    "    try:\n",
    "        tables = pd.read_html(url, attrs={'class': 'infobox'})\n",
    "        assert len(tables) == 1, f\"{url} should only have 1 table, it has {len(tables)}\"   # sanity check: we should have just 1 table\n",
    "        infotable = tables[0]    # pull the first table into a DataFrame\n",
    "        ret = ''                 # initialize an empty string for our return value\n",
    "        x = infotable.set_index(infotable.columns[0]).loc['Aliases'] # setting the index on column 0 will allow us to use .loc to look up the value of 'Aliases'\n",
    "        ret = x.values[0]\n",
    "    except e:\n",
    "        print(\"Problem with \" + url)\n",
    "        print(e.stacktrace())\n",
    "        ret = 'None'\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "legolas_url = 'https://en.wikipedia.org/wiki/Legolas'\n",
    "get_aliases(legolas_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so good.  It seems to work.  Now let's set up a DataFrame with a bunch of URLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://en.wikipedia.org/wiki/Gimli_(Middle-earth)',\n",
    "        'https://en.wikipedia.org/wiki/Frodo_Baggins',\n",
    "        'https://en.wikipedia.org/wiki/Legolas',\n",
    "        'https://en.wikipedia.org/wiki/Peregrin_Took',\n",
    "        'https://en.wikipedia.org/wiki/Meriadoc_Brandybuck']\n",
    "names = ['Gimli',\n",
    "         'Frodo',\n",
    "         'Legolas',\n",
    "         'Pippin',\n",
    "         'Meriadoc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df = pd.DataFrame()\n",
    "url_df['name'] = names\n",
    "url_df['url'] = urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pythonic way of iterating through each of those rows would involve the use of some sort of ```for``` loop.  In pandas,\n",
    "however, you can use the ```apply``` function to process an entire column!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df['url'].apply(get_aliases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take the resulting Series and assign it to a new column in our DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df['aliases'] = url_df['url'].apply(get_aliases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just put the ```url_df``` DataFrame aside for now.  We'll return to it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrames and Exploring Indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some lists of data that we can use to construct a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['Gandalf',\n",
    "         'Gimli',\n",
    "         'Frodo',\n",
    "         'Legolas',\n",
    "         'Bilbo',\n",
    "         'Sam',\n",
    "         'Pippin',\n",
    "         'Boromir',\n",
    "         'Aragorn',\n",
    "         'Galadriel',\n",
    "         'Meriadoc',\n",
    "         'Lily']\n",
    "races = ['Maia',\n",
    "         'Dwarf',\n",
    "         'Hobbit',\n",
    "         'Elf',\n",
    "         'Hobbit',\n",
    "         'Hobbit',\n",
    "         'Hobbit',\n",
    "         'Man',\n",
    "         'Man',\n",
    "         'Elf',\n",
    "         'Hobbit',\n",
    "         'Hobbit']\n",
    "magic = [10, 1, 4, 6, 4, 2, 0, 0, 2, 9, 0, np.NaN]\n",
    "aggression = [7, 10, 2, 5, 1, 6, 3, 8, 7, 2, 4, np.NaN]\n",
    "stealth = [8, 2, 5, 10, 5, 4, 5, 3, 9, 10, 6, np.NaN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "races"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few different ways to construct a DataFrame.  We can either use an empty constructor and assign Series:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"magenta\"> Q5: (2 points) Construct a dataframe with 5 columns (names, races, magic, aggression, and stealth) using the lists above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "knowledge_building": {
     "cell_groupid": "c04-q4",
     "cell_type": "rcode_answer"
    }
   },
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could have set things up with a dict (some of you may have already done it this way):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'name': names,\n",
    "        'race': races,\n",
    "        'magic': magic,\n",
    "        'aggression': aggression,\n",
    "        'stealth': stealth}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the index on the resulting DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set the index to something more useful than the default RangeIndex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_indexed = df.set_index('name')\n",
    "name_indexed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we take a look at the results, we see that we have a pandas Index instead of a RangeIndex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_indexed.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_indexed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the name Series as the index allows us to do things like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_indexed.loc['Aragorn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now recall the Hierarchical indexing from the readings. _(Section 8.1 McKinney)_ We can pass a list of column names to set_index to create a Hierarchical Index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_racename_indexed = df.set_index(['race', 'name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_racename_indexed.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will allow us to get a DataFrame that matches a value on the outer index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_racename_indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_racename_indexed.loc['Hobbit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the index on a Series to match the outer index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_racename_indexed['magic'].loc['Hobbit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or both indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_racename_indexed['magic'].loc['Hobbit', 'Frodo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or just the inner index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_racename_indexed['magic'].loc[:, 'Frodo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"magenta\"> Q6: (2 points) Using .loc find how much aggression Legolas, an Elf, has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "knowledge_building": {
     "cell_groupid": "c04-q4",
     "cell_type": "rcode_answer"
    }
   },
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking and Unstacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.set_index('race')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking takes \"wide\" data and makes it \"taller\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(['race']).stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we call reset_index on the resulting Series, we get the following DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(['race']).stack().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column names in the above DataFrame aren't particularly helpful, so we can rename them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(['race']).stack().reset_index().rename(\n",
    "    columns={'level_0': 'ID', 'level_1': 'variable', 0: 'value'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do the opposite of stacking by using the ```unstack``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stacked = df.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stacked.unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why would we want to stack or unstack?  It depends on what sorts of analyses we want to do \"downstream\".  It's also the basis for pivoting, melting, and pivot tables, which we'll cover later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's say we have another CSV file that contains URLs to Wikipedia pages for some of the LOTR characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "urls = pd.read_csv('https://github.com/umsi-data-science/data/raw/main/lotr_wikipedia.csv')\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the rows are \"aligned\", so we can use the ```concat``` function to concatenate the two DataFrames.\n",
    "Note that we specify the axis to be the columns.  The default is to concatenate by rows, which isn't what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df, urls], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's great, and it's consistent with what we've used in previous classes.  But what happens if the \n",
    "rows in the two DataFrames don't match up?  Let's load another file that has a slightly different\n",
    "sequence of rows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"magenta\"> Q7: (2 points) Construct a dataframe called ```urls_wrong_order``` with https://github.com/umsi-data-science/data/raw/main/lotr_wikipedia_wrong_order.csv and concat it with df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_wrong_order # this should return the dataframe you created in the previous cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a closer look at the name and url columns.  Something's not quite right.\n",
    "\n",
    "We can work around that by using the appropriate indexing and then using the SQL-like ```merge``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names = df.set_index('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_wrong_order_names = urls_wrong_order.set_index('name')\n",
    "urls_wrong_order_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names.join(urls_wrong_order_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df,urls_wrong_order],axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_wrong_order.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_wrong_order['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.merge(urls_wrong_order, on='name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add a few additional URLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_extras = pd.read_csv(\"https://github.com/umsi-data-science/data/raw/main/lotr_wikipedia_extras.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's use concat to add the new entries to the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_complete = pd.concat([urls,urls_extras])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "urls_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got a complete (for our purposes) list of URLs, let's use that DataFrame and our original\n",
    "one to demonstrate the different types of ```join```s.\n",
    "\n",
    "By default, ```join``` uses a left join, which means the all the values from the \"left\"\n",
    "side are used, whether or not there's a corresponding entry from the \"right\" side.  In the example \n",
    "below, note that the url value for \"Lily\" is \"NaN\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.merge(urls_wrong_order, on='name', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"opposite\" of a left join is, perhaps unsurprisingly, a \"right\" join, in which\n",
    "all the values from the \"right\" side are used, whether or not a corresponding\n",
    "value from the \"left\" side exists. Note in the following example that \"Lily\" has\n",
    "disappeared, and Treebeard and Elrond lack information about \"race\", \"magic\", \"aggression\", and \"stealth\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.merge(urls_complete, on='name', how='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to \"left\" and \"right\" joins, we have \"outer\" joins, which include\n",
    "values from both the \"left\" and \"right\" DataFrames, regardless of whether\n",
    "there are corresponding values in the other DataFrame.  Note that all of \n",
    "\"Lily\", \"Treebeard\" and \"Elrond\" are present in the following DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.merge(urls_complete, on='name', how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, there are \"inner\" joins, which include only those values that exist in both the \"left\" and \"right\" DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.merge(urls_complete, on='name', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it's nice to know how a particular row got added to the resulting DataFrame.  Using ```indicator=True```\n",
    "allows us to examine this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.merge(urls_complete, how='outer', indicator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll note that we used the ```merge``` function from the DataFrame and passed in the other DataFrame as an argument.\n",
    "You can also call the ```merge``` function from pandas directly and pass it the two DataFrames you are merging:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pivot 1](http://www.datasciencemadesimple.com/wp-content/uploads/2017/09/join-or-merge-in-python-pandas-1.png \"pivots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df, urls_complete, how='outer', indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"magenta\">Q8: (4 points) Join the ```url_df``` DataFrame (that contains aliases) to the ```df``` DataFrame using an appropriate merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation and Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some more basic pandas functionality\n",
    "One of the nice things about pandas is that it simplifies many common operations on datasets.  Let's load yet another LOTR dataset, \n",
    "this time using StringIO.  StringIO allows us to create a string that's then available as a file!\n",
    "\n",
    "Why would you want to do this?  Sometimes it's easier to just paste data right into a Jupyter notebook (or python script)\n",
    "than it is to create another CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "LOTRDATA=StringIO(\"\"\"name,race,gender,magic,aggression,stealth\n",
    "Gandalf,Maia,Male,10.0,7.0,8.0\n",
    "Gimli,Dwarf,None,1.0,10.0,2.0\n",
    "Frodo,Hobbit,Male,4.0,2.0,5.0\n",
    "Legolas,Elf,Male,6.0,5.0,10.0\n",
    "Bilbo,Hobbit,Male,4.0,1.0,5.0\n",
    "Sam,Hobbit,Male,2.0,6.0,4.0\n",
    "Pippin,Hobbit,Male,0.0,3.0,5.0\n",
    "Boromir,Human,Male,0.0,8.0,3.0\n",
    "Aragorn,Human,Male,2.0,7.0,9.0\n",
    "Galadriel,Elf,Female,9.0,2.0,10.0\n",
    "Lily,Hobbit,Female,,,\n",
    "Meriadoc,Hobbit,Male,,4.0,6.0\n",
    "Melian,Maia,Female,10.0,5.0,9.0\n",
    "Idril,Elf,Female,8.0,,8.0\n",
    "\"\"\")\n",
    "\n",
    "lotr = pd.read_csv(LOTRDATA, index_col=None)\n",
    "lotr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a DataFrame, we can get some basic statistics about it using the ```describe()``` function.\n",
    "Note that ```describe()``` only returns values for numeric columns.  Note too that it returns another DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lotr.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(lotr.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pivots and Pivot Tables\n",
    "The following cells are based on: \n",
    "http://nikgrozev.com/2015/07/01/reshaping-in-pandas-pivot-pivot-table-stack-and-unstack-explained-with-pictures/ , which is\n",
    "one of the best guides to pivots, pivot tables, stacking and unstacking that I've encountered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration purposes, let's create the same DataFrame that Nikolay Grozev uses in his tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from pandas import DataFrame\n",
    "\n",
    "table = OrderedDict((\n",
    "    (\"Item\", ['Item0', 'Item0', 'Item1', 'Item1']),\n",
    "    ('CType',['Gold', 'Bronze', 'Gold', 'Silver']),\n",
    "    ('USD',  ['1$', '2$', '3$', '4$']),\n",
    "    ('EU',   ['1€', '2€', '3€', '4€'])\n",
    "))\n",
    "metal = DataFrame(table)\n",
    "metal\n",
    "\n",
    "# create the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# make a table of items (rows) and costs (USD) \n",
    "# for each in gold and bronze\n",
    "metal.pivot(index='Item',columns='CType',values='USD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the image below... we're telling Pandas to take the table above, create a row for every item. \n",
    "This is done by setting index to Item (the column in the original table that contains item names)\n",
    "We then are telling pandas we want to create a column for every unique element in the\n",
    "original CType column.  And finally, we want the value in the cells to be the value from the USD\n",
    "column in the original table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![pivot 1](http://nikgrozev.com/images/blog/Reshaping%20in%20Pandas%20-%20Pivot%20Pivot-Table%20Stack%20and%20Unstack%20explained%20with%20Pictures/pivoting_simple1.png \"pivots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"magenta\">Q9: (2 points) Using pivot and the dataframe below make a table with each person as the index, their purchased items as columns, and the price they paid as the values.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1 = OrderedDict((\n",
    "    (\"Person\", ['Scott', 'Julie', 'Shiyan', 'Julie','Scott','Julie', 'Shiyan', 'Julie','Scott', 'Julie']),\n",
    "    ('Item Purchased',['Kit-Kat','Mango', 'Twix', 'M&Ms', 'Ferrero Rocher','Apple', 'Watermelon', 'Pineapple', 'Snickers','Pear']),\n",
    "    ('Price Paid',  ['$1','$2', '$1.5', '$2', '$1.5','$1','$1','$2', '$1.5', '$2']),\n",
    "    ))\n",
    "fav = DataFrame(table1)\n",
    "fav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Access the USD cost of Item0 for Gold customers...\n",
    "\n",
    "First we find the row for Item0/Gold and then we select the USD column and pull out the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "result = metal[((metal.Item == 'Item0') & (metal.CType == 'Gold'))].USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same thing on pivoted table. Here we pull out the row for Item0, grab the Gold column and print the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[p.index == 'Item0'].Gold.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now pivot by multiple columns, I want USD and EU prices. It returns a hierarchical index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metal.pivot(index='Item',columns='CType')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Access the USD cost of Item0 for Gold customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "p = metal.pivot(index='Item',columns='CType')\n",
    "p.USD[p.USD.index == 'Item0'].Gold.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What happens if there is a collision? \n",
    "See the problem?  There are two Item0/Golds:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![pivot 2](http://nikgrozev.com/images/blog/Reshaping%20in%20Pandas%20-%20Pivot%20Pivot-Table%20Stack%20and%20Unstack%20explained%20with%20Pictures/pivoting_simple_error.png \"pivots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up another DataFrame to demonstrate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "table = OrderedDict((\n",
    "    (\"Item\", ['Item0', 'Item0', 'Item0', 'Item1']),\n",
    "    ('CType',['Gold', 'Bronze', 'Gold', 'Silver']),\n",
    "    ('USD',  ['1$', '2$', '3$', '4$']),\n",
    "    ('EU',   ['1€', '2€', '3€', '4€'])\n",
    "))\n",
    "metal = DataFrame(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will generate an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = metal.pivot(index='Item', columns='CType', values='USD')\n",
    "# will return an error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## pivot_tables is your friend\n",
    "![pivot 1](http://nikgrozev.com/images/blog/Reshaping%20in%20Pandas%20-%20Pivot%20Pivot-Table%20Stack%20and%20Unstack%20explained%20with%20Pictures/pivoting_table_simple1.png \"pivots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create yet another DataFrame to play with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "table = OrderedDict((\n",
    "    (\"Item\", ['Item0', 'Item0', 'Item0', 'Item1']),\n",
    "    ('CType',['Gold', 'Bronze', 'Gold', 'Silver']),\n",
    "    ('USD',  [1, 2, 3, 4]),\n",
    "    ('EU',   [1.1, 2.2, 3.3, 4.4])\n",
    "))\n",
    "metal = DataFrame(table)\n",
    "metal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "pivot_table is  a bit different than pivot... It's the same with the first part\n",
    "index, columns, values remain the same as before BUT we added a rule (aggfunc)\n",
    "that says: whey you hit a conflict, the way to resolve it is X (in this case\n",
    "x is the \"mean\"... so find the mean of the two numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "p = metal.pivot_table(index='Item',columns='CType',values='USD',aggfunc=np.mean)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could have also resolved the conflict in other ways.  Here we tell it to take the \"min\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "p = metal.pivot_table(index='Item',columns='CType',values='USD',aggfunc=np.min)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"magenta\">Q10: (4 points) Using pivot_table and the dataframe below make a table with each person as the index, their purchased items as columns, and the sum of the price they paid as the values (sometimes people bought more than one of the same item).</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1 = OrderedDict((\n",
    "    (\"Person\", ['Scott', 'Julie', 'Shiyan', 'Julie',\n",
    "                'Scott','Julie', 'Shiyan', 'Julie','Scott', \n",
    "                'Julie','Scott', 'Julie', 'Shiyan', 'Julie',\n",
    "                'Scott','Julie',]),\n",
    "    ('Item Purchased',['Kit-Kat','Mango', 'Twix', 'M&Ms', 'Ferrero Rocher',\n",
    "                       'Apple', 'Watermelon', 'Pineapple', 'Snickers','Pear','Kit-Kat','Mango', 'Twix', 'M&Ms', 'Ferrero Rocher',\n",
    "                       'Apple']),\n",
    "    ('Price Paid (USD)',  [1,2, 1.5, 2, 1.5,1,1,2, 1.5, 2,\n",
    "                    1,2, 1.5, 2, 1.5,1]),\n",
    "    ))\n",
    "fav = DataFrame(table1)\n",
    "fav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Pivots are a specific form of stack/unstack (remember those?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pivot 1](http://nikgrozev.com/images/blog/Reshaping%20in%20Pandas%20-%20Pivot%20Pivot-Table%20Stack%20and%20Unstack%20explained%20with%20Pictures/stack-unstack1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A worked example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# to start let's make a fake dataset: sales of fruit across US states.\n",
    "# Don't worry about the details here, but basically we'll pretend\n",
    "# this string is a CSV file and use the standard loading ops\n",
    "from io import StringIO\n",
    "\n",
    "TESTDATA=StringIO(\"\"\"State,Retailer,Fruit,Sales\n",
    "MI,Walmart,Apple,100\n",
    "MI,Wholefoods,Apple,150\n",
    "MI,Kroger,Orange,180\n",
    "CA,Walmart,Apple,220\n",
    "CA,Wholefoods,Apple,180\n",
    "CA,Safeway,Apple,220\n",
    "CA,Safeway,Orange,110\n",
    "NY,Walmart,Apple,90\n",
    "NY,Walmart,Orange,80\n",
    "NY,Wholefoods,Orange,120\n",
    "\"\"\")\n",
    "\n",
    "fruit = pd.read_csv(TESTDATA, index_col=None)\n",
    "fruit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## (a) What is the total sales for each state?\n",
    "This requires us to group by state, and aggregate sales by taking the sum.\n",
    "\n",
    "The easiest way of doing this if to use `groupby`\n",
    "\n",
    "If you execute groupby on the dataframe what you'll get back is an object called DataFrameGroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fruit.groupby('State')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On its own it's a bit useless... it just keeps track of which rows should go into each \"pile\" (where pile here means a unique group for each state)\n",
    "\n",
    "If we ask this object to describe itself, you can see what is inside notice that it threw away all the other columns because they were not numerical.  Only \"Sales\" which is a number, was kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fruit.groupby('State').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, if we had another numerical column, let's call it \"Sales2,\" that column would also be kept.  Let's make a fruit2 DataFrame so you can see that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "TESTDATA=StringIO(\"\"\"State,Retailer,Fruit,Sales,Sales2\n",
    "MI,Walmart,Apple,100,10\n",
    "MI,Wholefoods,Apple,150,20\n",
    "MI,Kroger,Orange,180,30\n",
    "CA,Walmart,Apple,220,20\n",
    "CA,Wholefoods,Apple,180,40\n",
    "CA,Safeway,Apple,220,30\n",
    "CA,Safeway,Orange,110,20\n",
    "NY,Walmart,Apple,90,40\n",
    "NY,Walmart,Orange,80,20\n",
    "NY,Wholefoods,Orange,120,60\n",
    "\"\"\")\n",
    "\n",
    "fruit2 = pd.read_csv(TESTDATA, index_col=None)\n",
    "fruit2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fruit2.groupby(\"State\").describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To actually make use of the groupby, we need to tell pandas what to use to measure what's in each group. In other words, I've created a pile for California, a pile for Michigan, and a pile for New York.  I want a number to what's *inside* each pile.  I could ask for the \"size\" (so how many rows are in each pile), or I could calculate some mathematical function.  For example, if I wanted to know the total sales, I would call \"sum.\"  What happens is pandas goes through every pile, looks at every \"row\" inside that pile and, for all numerical properties, calculated something.  In this case it's sum... it adds up everything.  So in our original table we had three items for Michigan (Walmart, Kroger, Wholefoods).  This is our Michigan pile.  We then look at numerical properties for Walmart, Kroger, and Wholefoods. In this case Sales.  Because we are using sum() that means add the sales of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# What are the total sales for each state?\n",
    "fruit.groupby('State')['Sales'].sum()  # instead of size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What just happend? A couple of things:\n",
    "- `groupby()` got first executed on `df`, returning an `DataFrameGroupBy` object. This object itself is useless unless coupled with an aggregation function, such as `sum()`, `mean()`, `max()`, `apply()`. \n",
    "- Then, `sum()` got executed on the `DataFrameGroupBy` object, generating the `DataFrame` object you see above. Notice how the table looks different than the original DataFrame `df`? Here are the differences:\n",
    "  - The `State` column now becomes the index of the DataFrame. The string \"State\" is the name of the index. Notice how the index name is displayed on a lower level than column names.\n",
    "  - Since we performed a `groupby` operation by `State`, so only the unique values of `State` are kept as index.\n",
    "  - Among the other columns, Retailer, Fruit, and Sales, only Sales is kept in the result table. This is because the aggregation function `sum()` only knows how to aggregate numerical values. And only Sales is a numerical column. The other columns are hence dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"magenta\">Q11: (2 points) Using groupby, which Retailer had the highest total Sales and how much was that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## (b) What is the total sales for each state for each fruit?\n",
    "This requires us to perform `groupby` on two columns. So, we provide a list of column names to the `groupby` function.\n",
    "\n",
    "Don't forget that an aggregation function needs to follow the `groupby` function in order to generate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# What is the total sales for each state for each fruit?\n",
    "fruit.groupby(['State', 'Fruit'])['Sales'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How is this DataFrame different from the previous one?\n",
    "\n",
    "The biggest different is that this DataFrame has what is called a `MultiIndex` (or hierarchical index), as opposed to a simple index. In this table, the left two \"columns\" are not columns but actually part of the `MultiIndex`, and the `Sales` is the single real \"column\" in the DataFrame. (Running out of terminologies here...)\n",
    "\n",
    "The hierarchical index can be organized in an alternative way if we swapped the order of State and Fruit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fruit.groupby(['Fruit', 'State'])['Sales'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## (c) Which state has the maximum total sales?\n",
    "This question is not asking about the maximum value, but rather which state holds that maximum. There are multiple ways to do it. A principled way is to use `idxmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which state has the maximum total sales?\n",
    "fruitSalesByState = fruit.groupby('State')['Sales'].sum()\n",
    "print(fruitSalesByState)\n",
    "max_state = fruitSalesByState.idxmax()\n",
    "print(\"The state with the maximum sales is: \",max_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "What if I want to display the maximum value alongside the state? Well, we can use that returned label to _select_ the corresponding row from the original DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fruitSalesByState.loc['CA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A less efficient but more intuitive way of doing the same thing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which state has the maximum total sales for apples?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Which state has the maximum total sales for apples?\n",
    "# give me apple sellers\n",
    "apples = fruit[fruit.Fruit == 'Apple']\n",
    "apples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggr. by state\n",
    "applesByState = apples.groupby('State')['Sales'].sum()\n",
    "applesByState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applesByState.idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying what we learned to the LOTR data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the average values for magic, aggression, and stealth for each race?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "LOTRDATA=StringIO(\"\"\"name,race,gender,magic,aggression,stealth\n",
    "Gandalf,Maia,Male,10.0,7.0,8.0\n",
    "Gimli,Dwarf,None,1.0,10.0,2.0\n",
    "Frodo,Hobbit,Male,4.0,2.0,5.0\n",
    "Legolas,Elf,Male,6.0,5.0,10.0\n",
    "Bilbo,Hobbit,Male,4.0,1.0,5.0\n",
    "Sam,Hobbit,Male,2.0,6.0,4.0\n",
    "Pippin,Hobbit,Male,0.0,3.0,5.0\n",
    "Boromir,Human,Male,0.0,8.0,3.0\n",
    "Aragorn,Human,Male,2.0,7.0,9.0\n",
    "Galadriel,Elf,Female,9.0,2.0,10.0\n",
    "Lily,Hobbit,Female,,,\n",
    "Meriadoc,Hobbit,Male,,4.0,6.0\n",
    "Melian,Maia,Female,10.0,5.0,9.0\n",
    "Idril,Elf,Female,8.0,,8.0\n",
    "\"\"\")\n",
    "\n",
    "lotr = pd.read_csv(LOTRDATA, index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lotr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"magenta\">Q12: (4 points) Create a pivot table showing the maximum values of magic using gender for columns and race for rows.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"magenta\">END OF NOTEBOOK</font>\n",
    "## Remember to submit this file in HTML and IPYNB formats via Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "90fd63f6819c340b438e5c293ddfb31e39be622c688d56bb62c76ae0c0566a54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
